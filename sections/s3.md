# [S3 & Glacier](../README.md)

## S3

* `Buckets` _(directories)_ are storing `objects` _(files)_
* Buckets' name must be globally unique
* S3 is a regional service
* Naming conventions
	* No uppercase
	* No underscore
	* 3-63 characters long
	* Not an IP
	* Must start with lowercase letter or number
* `Objects` have a key
	* The key is the full path
* There is no concept of directories (but in the UI does look like it)
* `ObjectValues`
	* Content of the body
	* Max size is 5 TB
	* If file is larger than 5 GB, must use `multi-part upload`
* `Metadata`
	* Key-Value pairs
* `Tags`
	* Key-value pairs
	* Maximum 10 tags
* `VersionID`
* Open a file
	* Right click -> Open: Uses correct permissions
	* Object link -> Access denied (as it is a public link)

### Storage tiers

* `Standard`: General purpose
	* 99,99% Availability
	* 11 9 durability (99,999999999%) across multiple AZ
	* Can sustain 2 az outage
* `Reduced redundancy storage`: DEPRECATED
	* 99,99% Durability
* `Infrequent Access (IA)`:
	* Not accessed regularly but when it does, need quick access
	* 99,99% Availability
	* 11 9 durability (99,999999999%) across multiple AZ
	* Low cost compared to Standard
	* Fee when accessing objects
* `One Zone Infrequent Access (One zone IA)`:
	* Same as IA but only in 1 AZ
	* If AZ is DESTROYED (not down), then data is lost
	* 99,95% Availability
	* Lower cost than IA (20% )
* `Intelligent Tiering`: announced in 2018
	* Best of all, as it can move object to the correct tier
	* Small monthly fee for auto-tiering
	* Only 99.9% Availability though
* `Glacier`:

### Lifecycle rules

* Set of rules to __move data between different tiers__ to save costs
	* Transition actions
		* Move objects into another tier
	* Expiration actions
		* Delete objects after a certain period
	* Delete incomplete multipart uploads

### Storage Class Analytics

* Setup analytics to __help determine when to transition objects to different tiers__
* Does NOT work for OneZone IA or Glacier
* Daily report
* First report takes 1-2 days

### Versioning

* Have to enable at bucket settings
* Same key overwrite will increment version, and it will keep all versions
* Any file that is not versioned prior to enabling versioning will have `Version ID: null`
* Deleting a versioned file: __Adds a delete marker__, won't delete previous versions 
* Can protect against hackers, cause each version is a separate file
* Cannot delete bucket unless all objects AND all versions are deleted first
* __If an object has millions of versions, it can affect performance!__

### Cross Region Replication

* Asynchronously replicate
* __Must enable source and destination bucket versioning!__
* Buckets can be in different accounts
* Must give proper IAM permissions

### S3 Inventory

* Helps to manage storage
* __Audit and report on replication and encryption status of your objects__
* Data for the Source bucket has to go to a Target bucket (need to set up IAM permissions)

### Security

* `User based`
	* __IAM policies__
* `Resource based`
	* __Bucket policies__ (bucket wide rules)
		* Allows cross account access
	* __Object Access Control List__ (`Object ACLs`): Finer grain access control
	* __Bucket Access Control List__ (`Bucket ACLs`): Less common
* Networking
	* Supports VPC endpoints (Instances in VPC without internet access can still use S3)
* Logging and Audit features
	* S3 access logs can be stored in another S3 bucket
	* API calls can be logged in CloudTrail
* User security
	* MFA required can be required to delete versioned buckets
	* Signed URLs: URL that valid only for a limited time

#### Encryption for objects

* Methods
	* `SSE-S3`
		* Using AWS S3 managed keys, handled by AWS seamlessly
		* Object encrypted at server side
		* AES-256
		* Must set header when uploading: `"x-amz-server-side-encryption":"AES256"`
	* `SSE-KMS`
		* KMS to manage encryption keys
		* Similar to SSE-S3, just using KMS managed keys instead of S3
		* Advantages:
			* User control
			* Audit trail
		* Must set header when uploading: `"x-amz-server-side-encryption":"aws:kms"`
		* Still seamless
	* `SSE-C`
		* You manage your encryption keys
		* Data keys are fully managed by the customer
		* S3 won't store the keys
		* Headers required
			* `x-amz-server-side​-encryption​-customer-algorithm`
			* `x-amz-server-side​-encryption​-customer-key-MD5`
			* `x-amz-server-side​-encryption​-customer-key`
		* __HTTPS must be used__
		* Encryption keys must be provided in the header for every request! 
		* Still seamless
	* `Client side encryption`
		* Client library such as AWS S3 Encryption Client
		* Clients must encrypt the data themselves before uploading to S3
		* Clients must decrypt the data themselves after downloading from S3
		* Customer fully manages the keys and encryption cycle
* Encryption at transit _(SSL/TLS)_
	* `HTTP endpoint`: not encrypted
	* `HTTPS endpoint`: encrypted
* Can set bucket option to encrypt all files even if the files doesn't specify encryption

#### MFA Delete

* First need to enable versioning on the bucket
* Only the bucket owner (root account) can enable/disable MFA-Delete
* Can only enable is via the CLI (as of now)
* Need to MFA before
	* Permanently deleting an object version
	* Suspend versioning
* DON'T need MFA for
	* Enabling versioning
	* Listing deleted versions
* If enabled, you can't do any permanent delete as the UI doesn't use MFA -> must do all permanent delete in the CLI

#### Bucket policy

* JSON based
	* `Resources`: Buckets or objects
	* `Actions`: Set of API actions
	* `Effect`: Allow or Deny
	* `Principal`: Account or user to apply the policy to
* Can do various things using it:
	* Cross account access
	* Force encryption on objects
	* Grant public access
* [AWS Bucket policy examples](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html)

#### Access Control List

* `READ`: List bucket objects / read objects and its metadata
* `READ_ACP`: Allows to read bucket/object ACL
* `WRITE`: Create, overwrite and delete any object in the bucket
* `WRITE_ACP`: Write, delete, create Object ACLs
* `FULL_CONTROL`: All previous policies

#### Access logs

* Any request will be logged (Can be from another account, access denied etc..)
* Can use Data analysis tools (Amazon Athena)
* Can use prefix in logging bucket
* Can take some time for the logs to show up

#### Default encryption Vs Bucket policies

* Old way: Bucket policy
* New way: Just enable default encryption on Bucket
* NOTE: Bucket policy evaluated before default encryption

### Static website

* Can host static website
* URL Format
	* `<bucket_name>.s3-website.<aws_region>.amazonaws.com`
	* `<bucket_name>.s3-website-<aws_region>.amazonaws.com`
* If you get `403` -> Make sure bucket policy allows __Public read__ on the bucket!
* Properties -> Enable static website
	* Set index document
* Permissions -> Public access settings
	* Enable public access
* Set bucket policy to allow public read

### CORS

* Cross Origin Resource Sharing
* If you request data from another S3 bucket, need to set CORS
* Allows to limit the number of websites that can request your files (~limits your costs)

### Pre-signed URL

* Can be generated via SDKs or CLI
* Default: valid for 1 hour (3600 seconds) `--expires-in [time_in_seconds]`
* The URL will inherit your permissions], so whoever uses the link will be able what you could
* `aws configure set default.s3.signature_version s3v4`
* Set `--region` appropriately

### Consistency model

* __Read after Write consistency for PUTs of new objects__
	* As soon as the object is written, we can access it
		* `PUT 200` -> `GET 200`
	* Except!
		* `GET 404` -> `PUT 200` -> `GET 404`, but eventually will be consistent
* __Eventual consistency for DELETEs and PUTs of existing objects__
	* Might get older version after updating it
		* `PUT 200` -> `PUT 200` -> `GET 200` (might be older version)
		* `DELETE 200` -> `GET 200`

## Glacier

* Low cost storage for archiving / backup
* Data is retained for a long time (10s of years)
* Alternative to on-premise magnetic tape storage
* 11 nine durability
* $0.004/GB + retrieval fee
* `Object` in other tiers = `Archive` in Glacier
* An archive can be up to 40 TB
* Archives are stored in `Vaults` (again, different naming convention)
* Operations
	* `Upload` (single, or multi part upload)
	* `Download`: Initiate a retrieval request, wait for file to be ready, limited time to download!
	* `Delete` (Use SDK or Glacier REST API)
* Restore links have an expiry date
* 3 retrieval options:
	* __Expedited__: 1 to 5 minutes retrieval, `$0.03/GB` + `$0.01/request`, Capacity units
	* __Standard__: 3 to 5 hours retrieval, `$0.01/GB` + `$0.05/ 1000 request`
	* __Bulk__: 5 to 12 hours retrieval, `$0.025/GB` + `$0.025/ 1000 request`

### Vault lock & policies

* Vault Collection of archives
* Each Vault has:
	* 1 Vault __access policy__ (JSON), similar to bucket policies
	* 1 Vault __lock policy__
		* Immutable
		* A policy that has been locked. E.g.: Can never be changed, removed
			* Example 1: `WORM policy`: Write Once, Read Many
			* Example 2: Cannot delete archives that are younger than 1 year
		* When creating a lock policy, you get a LockID: 24 hours to confirm lock policy

## [Back](../README.md)